{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks applications to fraud detection\n",
    "## Comparing alternative methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are one of the most relevant learning methods currently available, and their widespread application is understood by theoretical robustness, flexible architecture design, and strong expected predictive accuracy.\n",
    "<br>\n",
    "<br>\n",
    "The main objective of this study is to develop a neural network application to fraud detection, and mainly to construct and implement a strategy for hyper-parameter tuning, since this learning method requires a proper definition of a large set of parameters in order to result in a competitive performance.\n",
    "<br>\n",
    "<br>\n",
    "Previously to empirical inquirements, it is necessary to review all details concerning neural networks structure, fitting, and specification, which will base experiments design and tests implementation. So, the theoretical presentation of this notebook will be followed by an empirical stage of tests in which hyper-parameters will be defined to improve neural networks predictive accuracy, after which the best specification obtained should be opposed to alternative learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After estimating models for different learning methods, presenting and discussing their results, this notebook compares the performance of all tested methods based on different statistical metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "1. [Libraries](#libraries)<a href='#libraries'></a>.\n",
    "2. [Settings](#settings)<a href='#settings'></a>.\n",
    "3. [Importing data](#imports)<a href='#imports'></a>.\n",
    "4. [Comparing performance of alternative methods](#comparing_performance)<a href='#comparing_performance'></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "# print(__version__) # requires version >= 1.9.0\n",
    "\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='settings'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare whether to export results:\n",
    "export = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imports'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with information on model structure and performance (neural networks):\n",
    "os.chdir('/home/matheus_rosso/Arquivo/Materiais/Codes/neural_nets/')\n",
    "\n",
    "model_assessment = {}\n",
    "\n",
    "with open('Datasets/model_assessment.json') as json_file:\n",
    "    model_assessment['NN'] = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with information on model structure and performance (alternative methods):\n",
    "for m in ['LR', 'SVM', 'GBM']:\n",
    "    with open('Datasets/model_assessment_{0}.json'.format(m)) as json_file:\n",
    "        model_assessment[m] = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparing_performance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing performance of alternative methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for performance metrics by learning method:\n",
    "avg_roc_auc = [\n",
    "    model_assessment['NN']['1615253754']['performance_metrics']['avg_roc_auc'],\n",
    "    model_assessment['LR']['1615061729']['performance_metrics']['avg_roc_auc'],\n",
    "    model_assessment['SVM']['1615402205']['performance_metrics']['avg_roc_auc'],\n",
    "    model_assessment['GBM']['1615580184']['performance_metrics']['avg_roc_auc']\n",
    "]\n",
    "\n",
    "avg_avg_prec_score = [\n",
    "    model_assessment['NN']['1615253754']['performance_metrics']['avg_avg_prec_score'],\n",
    "    model_assessment['LR']['1615061729']['performance_metrics']['avg_avg_prec_score'],\n",
    "    model_assessment['SVM']['1615402205']['performance_metrics']['avg_avg_prec_score'],\n",
    "    model_assessment['GBM']['1615580184']['performance_metrics']['avg_avg_prec_score']\n",
    "]\n",
    "\n",
    "avg_brier_score = [\n",
    "    model_assessment['NN']['1615253754']['performance_metrics']['avg_brier_score'],\n",
    "    model_assessment['LR']['1615061729']['performance_metrics']['avg_brier_score'],\n",
    "    model_assessment['SVM']['1615402205']['performance_metrics']['avg_brier_score'],\n",
    "    model_assessment['GBM']['1615580184']['performance_metrics']['avg_brier_score']\n",
    "]\n",
    "\n",
    "std_roc_auc = [\n",
    "    model_assessment['NN']['1615253754']['performance_metrics']['std_roc_auc'],\n",
    "    model_assessment['LR']['1615061729']['performance_metrics']['std_roc_auc'],\n",
    "    model_assessment['SVM']['1615402205']['performance_metrics']['std_roc_auc'],\n",
    "    model_assessment['GBM']['1615580184']['performance_metrics']['std_roc_auc']\n",
    "]\n",
    "\n",
    "std_avg_prec_score = [\n",
    "    model_assessment['NN']['1615253754']['performance_metrics']['std_avg_prec_score'],\n",
    "    model_assessment['LR']['1615061729']['performance_metrics']['std_avg_prec_score'],\n",
    "    model_assessment['SVM']['1615402205']['performance_metrics']['std_avg_prec_score'],\n",
    "    model_assessment['GBM']['1615580184']['performance_metrics']['std_avg_prec_score']\n",
    "]\n",
    "\n",
    "std_brier_score = [\n",
    "    model_assessment['NN']['1615253754']['performance_metrics']['std_brier_score'],\n",
    "    model_assessment['LR']['1615061729']['performance_metrics']['std_brier_score'],\n",
    "    model_assessment['SVM']['1615402205']['performance_metrics']['std_brier_score'],\n",
    "    model_assessment['GBM']['1615580184']['performance_metrics']['std_brier_score']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>avg_roc_auc</th>\n",
       "      <th>std_roc_auc</th>\n",
       "      <th>avg_avg_prec_score</th>\n",
       "      <th>std_avg_prec_score</th>\n",
       "      <th>avg_brier_score</th>\n",
       "      <th>std_brier_score</th>\n",
       "      <th>ratio_roc_auc</th>\n",
       "      <th>ratio_avg_prec_score</th>\n",
       "      <th>ratio_brier_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.955409</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.505008</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>16508.084976</td>\n",
       "      <td>582.909927</td>\n",
       "      <td>1071.775044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>GBM</td>\n",
       "      <td>0.947797</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.441309</td>\n",
       "      <td>0.033076</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>239.292287</td>\n",
       "      <td>13.342267</td>\n",
       "      <td>19.752882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.946432</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.460111</td>\n",
       "      <td>0.012830</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>521.099463</td>\n",
       "      <td>35.862691</td>\n",
       "      <td>29.859895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.941386</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.489918</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>245019.126469</td>\n",
       "      <td>13948.539729</td>\n",
       "      <td>369.248814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method  avg_roc_auc  std_roc_auc  avg_avg_prec_score  std_avg_prec_score  \\\n",
       "1     LR     0.955409     0.000058            0.505008            0.000866   \n",
       "3    GBM     0.947797     0.003961            0.441309            0.033076   \n",
       "0     NN     0.946432     0.001816            0.460111            0.012830   \n",
       "2    SVM     0.941386     0.000004            0.489918            0.000035   \n",
       "\n",
       "   avg_brier_score  std_brier_score  ratio_roc_auc  ratio_avg_prec_score  \\\n",
       "1         0.008912         0.000008   16508.084976            582.909927   \n",
       "3         0.009877         0.000500     239.292287             13.342267   \n",
       "0         0.010527         0.000353     521.099463             35.862691   \n",
       "2         0.008749         0.000024  245019.126469          13948.539729   \n",
       "\n",
       "   ratio_brier_score  \n",
       "1        1071.775044  \n",
       "3          19.752882  \n",
       "0          29.859895  \n",
       "2         369.248814  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe with statistics of performance metrics by learning method:\n",
    "metrics = pd.DataFrame(data = {\n",
    "    'method': ['NN', 'LR', 'SVM', 'GBM'],\n",
    "    'avg_roc_auc': avg_roc_auc,\n",
    "    'std_roc_auc': std_roc_auc,\n",
    "    'avg_avg_prec_score': avg_avg_prec_score,\n",
    "    'std_avg_prec_score': std_avg_prec_score,\n",
    "    'avg_brier_score': avg_brier_score,\n",
    "    'std_brier_score': std_brier_score\n",
    "})\n",
    "\n",
    "for m in ['roc_auc', 'avg_prec_score', 'brier_score']:\n",
    "    metrics[f'ratio_{m}'] = metrics[f'avg_{m}']/metrics[f'std_{m}']\n",
    "\n",
    "metrics.sort_values(['avg_roc_auc', 'ratio_roc_auc'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
